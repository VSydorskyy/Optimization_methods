{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True,precision=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторна робота №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Цільова функція:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_function = lambda x: -x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Умови зупинки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$||x^{k+1} - x^k||\\leq\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_norm_stop(x_prev,x_cur,epselon):\n",
    "    return np.linalg.norm(x_prev-x_cur) < epselon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$||f(x^{k+1}) - f(x^k)||\\leq\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_abs_stop(func_prev,func_cur,epselon):\n",
    "    return abs(func_prev - func_cur) < epselon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$||f'(x^{k+1})||\\leq\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_abs_stop(grad,epselon):\n",
    "    return np.linalg.norm(grad)<epselon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector:\n",
    "    \n",
    "    def __init__(self, project_point, constraint, step_size, adaptive_beta, grad_step_size):\n",
    "        self.project_point = project_point\n",
    "        self.f = lambda x: np.linalg.norm(project_point - x)\n",
    "        self.constraint = constraint\n",
    "        self.x = self.constraint.initial_point\n",
    "        \n",
    "        self.f_value = self.f(self.x)\n",
    "        \n",
    "        self.grad_step_size = grad_step_size\n",
    "        self.step_size = step_size\n",
    "        self.adaptive_beta = adaptive_beta\n",
    "        self.initial_step_size = step_size\n",
    "        \n",
    "        self.grad = None\n",
    "    \n",
    "    def adaptive_step_size(self):\n",
    "        alpha = self.step_size\n",
    "        new_x = self.constraint.compose_x(self.x - alpha*self.grad)\n",
    "        \n",
    "        while (len(new_x) == 0) or (self.f_value < self.f(new_x)):\n",
    "            alpha = alpha * self.adaptive_beta\n",
    "            new_x = self.constraint.compose_x(self.x - alpha*self.grad)\n",
    "            \n",
    "        return alpha\n",
    "                  \n",
    "    def backward(self):\n",
    "        self.grad = self.constraint.derivative(self.x, self.project_point)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.grad = np.zeros(self.x.shape[0])\n",
    "        \n",
    "    def step(self):\n",
    "        \n",
    "        self.step_size = self.adaptive_step_size()\n",
    "        \n",
    "        # x_k+1 = x_k - alpha_k * f(x_k)'\n",
    "        self.x = self.constraint.compose_x(self.x - self.step_size * self.grad)\n",
    "        self.f_value = self.f(self.x)\n",
    "                \n",
    "                \n",
    "    def info(self):\n",
    "        print('Current x: {}'.format(self.x))\n",
    "        print('Current f(x): {}'.format(self.f_value))\n",
    "        print('Current grad: {}'.format(self.grad))\n",
    "        print('Step size: {}'.format(self.step_size))\n",
    "        print('Gradient step size: {}'.format(self.grad_step_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_on_set(set_boarders, target_point):\n",
    "    \n",
    "    projected_points = []\n",
    "    points_dist = []\n",
    "    for boarder in set_boarders:\n",
    "        pr = Projector(project_point=target_point,\n",
    "                       adaptive_beta=0.5,\n",
    "                       constraint=boarder,\n",
    "                       grad_step_size=0.01, \n",
    "                       step_size=1)\n",
    "        \n",
    "        eps = 0.00001\n",
    "        num_itter = 0\n",
    "        previous_x = pr.x + eps + 1\n",
    "\n",
    "        while not x_norm_stop(epselon=eps, x_cur=pr.x, x_prev=previous_x):\n",
    "            num_itter +=1\n",
    "\n",
    "            previous_x = pr.x\n",
    "            pr.zero_grad()\n",
    "            # Compute gradient and gesse matrix\n",
    "            pr.backward()\n",
    "            # x_k+1 = x_k + h_k\n",
    "            pr.step()\n",
    "\n",
    "        projected_points.append(pr.x)\n",
    "        points_dist.append(pr.f_value)\n",
    "        \n",
    "    return projected_points[np.argmin(points_dist)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Граничні умови"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x\\geq0$$<br>\n",
    "$$y\\geq0$$<br>\n",
    "$$(1-x)^3 - y\\geq0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConstraints_1(object):\n",
    "    def __init__(self):\n",
    "        self.initial_point = self.compose_x([0.5,0.5])\n",
    "    \n",
    "    def compose_x(self, initial_x):\n",
    "        \n",
    "        if initial_x[0] < 0. or initial_x[0] > 1. :\n",
    "            return np.array([])\n",
    "        else:\n",
    "            return np.array([initial_x[0], (1-initial_x[0])**3])\n",
    "        \n",
    "    def derivative(self, initial_x, project_point):\n",
    "        x_grad = ((initial_x[0]-project_point[0])\\\n",
    "                  - 3*(initial_x[1]-project_point[1])*((1-initial_x[0])**2))\\\n",
    "                  / np.linalg.norm(initial_x-project_point)\n",
    "        return np.array([x_grad,0])\n",
    "    \n",
    "class MyConstraints_2(object):\n",
    "    def __init__(self):\n",
    "        self.initial_point = self.compose_x([0.5,0.5])\n",
    "    \n",
    "    def compose_x(self, initial_x):\n",
    "        \n",
    "        if initial_x[1] < 0. or initial_x[1] > 1. :\n",
    "            return np.array([])\n",
    "        else:\n",
    "            return np.array([0, initial_x[1]])\n",
    "        \n",
    "    def derivative(self, initial_x, project_point):\n",
    "        y_grad = (initial_x[1]-project_point[1]) / np.linalg.norm(initial_x-project_point)\n",
    "        return np.array([0,y_grad])\n",
    "    \n",
    "class MyConstraints_3(object):\n",
    "    def __init__(self):\n",
    "        self.initial_point = self.compose_x([0.5,0.5])\n",
    "    \n",
    "    def compose_x(self, initial_x):\n",
    "        \n",
    "        if initial_x[0] < 0. or initial_x[0] > 1. :\n",
    "            return np.array([])\n",
    "        else:\n",
    "            return np.array([initial_x[0], 0])\n",
    "        \n",
    "    def derivative(self, initial_x, project_point):\n",
    "        x_grad = (initial_x[0]-project_point[0]) / np.linalg.norm(initial_x-project_point)\n",
    "        return np.array([x_grad,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_check_constraints(x):\n",
    "    return x[0] >= 0 and x[1] >=0 and (1-x[0])**3 - x[1] >=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Умовний оптимізатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, target_func, initial_x, step_size, grad_step_size,\n",
    "                 constraints, constraint_check,\n",
    "                 adaptive_beta=0, fastest_descent_eps=0):\n",
    "        self.f = target_func\n",
    "        self.x = initial_x\n",
    "        self.constraints = constraints\n",
    "        self.constraint_check = constraint_check\n",
    "        \n",
    "        self.f_value = self.f(self.x)\n",
    "        \n",
    "        self.grad_step_size = grad_step_size\n",
    "        self.step_size = step_size\n",
    "        self.adaptive_beta = adaptive_beta\n",
    "        self.initial_step_size = step_size\n",
    "        self.fastest_descent_eps = fastest_descent_eps\n",
    "        \n",
    "        self.grad = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def partial_deriv(f, x, h, var_num):\n",
    "        x_back, x_forward = x.copy(), x.copy()\n",
    "        \n",
    "        # Increase x_back in such a way: (x-h;y)\n",
    "        x_back[var_num] = x_back[var_num] - h \n",
    "        # Increase x_forward in such a way: (x+h;y)\n",
    "        x_forward[var_num] = x_forward[var_num] + h\n",
    "        \n",
    "        return (f(x_forward) - f(x_back))/(2*h)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compose_grad_vec(f, x, h):\n",
    "        # Compose vector from partial derivatives\n",
    "        return np.array([GradientDescent.partial_deriv(f,x,h,i) for i in range(x.shape[0])])\n",
    "    \n",
    "    @staticmethod\n",
    "    def adaptive_step_size(f, current_f_value, alpha, beta):\n",
    "        # decrease alpha_k until f(x_k) > f(x_k - x_k - alpha_k * f(x_k)')\n",
    "        while current_f_value < f(alpha):\n",
    "            alpha = alpha * beta\n",
    "            \n",
    "        return alpha\n",
    "    \n",
    "    @staticmethod\n",
    "    def gold_section_search(f, a, b, eps):\n",
    "        phi = 0.5 * (1.0 + 5.0**0.5)\n",
    "        \n",
    "        while abs(b-a) >= eps:\n",
    "            x_1 = b - (b-a)/phi\n",
    "            x_2 = a + (b-a)/phi\n",
    "            \n",
    "            if f(x_1) > f(x_2):\n",
    "                a = x_1\n",
    "            else:\n",
    "                b = x_2\n",
    "                \n",
    "        return (a+b)/2\n",
    "            \n",
    "        \n",
    "    def backward(self):\n",
    "        self.grad = GradientDescent.compose_grad_vec(self.f, self.x, self.grad_step_size) \n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.grad = np.zeros(self.x.shape[0])\n",
    "        \n",
    "    def step(self):\n",
    "        \n",
    "        # if beta > 0 we are using adaptive step_size\n",
    "        if self.adaptive_beta > 0:\n",
    "            self.step_size = GradientDescent.adaptive_step_size(f=lambda alpha: self.f(self.x - alpha * self.grad),\n",
    "                                                                current_f_value=self.f_value,\n",
    "                                                                alpha=self.initial_step_size, beta=self.adaptive_beta)\n",
    "        \n",
    "        # if fastest descent epselon > 0 we are using fastest descent algorithm\n",
    "        elif self.fastest_descent_eps > 0:\n",
    "            self.step_size = GradientDescent.gold_section_search(f=lambda alpha: self.f(self.x - alpha * self.grad),\n",
    "                                                                 a=0, b=self.initial_step_size, \n",
    "                                                                 eps=self.fastest_descent_eps)\n",
    "        \n",
    "        \n",
    "        # x_k+1 = x_k - alpha_k * f(x_k)'\n",
    "        self.x = self.x - self.step_size * self.grad\n",
    "        \n",
    "        if not self.constraint_check(self.x):\n",
    "            print('Projection')\n",
    "            print('old x: {}'.format(self.x))\n",
    "            self.x = project_on_set(self.constraints, self.x)\n",
    "            print('new x: {}'.format(self.x))\n",
    "            \n",
    "        self.f_value = self.f(self.x)\n",
    "                \n",
    "                \n",
    "    def info(self):\n",
    "        print('Current x: {}'.format(self.x))\n",
    "        print('Current f(x): {}'.format(self.f_value))\n",
    "        print('Current grad: {}'.format(self.grad))\n",
    "        print('Step size: {}'.format(self.step_size))\n",
    "        print('Gradient step size: {}'.format(self.grad_step_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_descent = GradientDescent(target_func=target_function,\n",
    "                               initial_x=np.array([0.2,0.2]),\n",
    "                               step_size=1,\n",
    "                               grad_step_size=0.00001,\n",
    "                               fastest_descent_eps=0.00001,\n",
    "                               constraints=[MyConstraints_1(),MyConstraints_2(),MyConstraints_3()],\n",
    "                               constraint_check=my_check_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Itteration: 1\n",
      "Projection\n",
      "old x: [1.1999952 0.2      ]\n",
      "new x: [0.9999991 0.       ]\n",
      "Current x: [0.9999991 0.       ]\n",
      "Current f(x): -0.9999990945967305\n",
      "Current grad: [-1.  0.]\n",
      "Step size: 0.9999951775621607\n",
      "Gradient step size: 1e-05\n",
      "\n",
      "Itteration: 2\n",
      "Projection\n",
      "old x: [1.9999943 0.       ]\n",
      "new x: [1. 0.]\n",
      "Current x: [1. 0.]\n",
      "Current f(x): -1.0\n",
      "Current grad: [-1.  0.]\n",
      "Step size: 0.9999951775621607\n",
      "Gradient step size: 1e-05\n",
      "\n",
      "Converged in 2 itterations\n"
     ]
    }
   ],
   "source": [
    "eps = 0.00001\n",
    "num_itter = 0\n",
    "previous_x = grad_descent.x + eps + 1\n",
    "\n",
    "while not x_norm_stop(epselon=eps, x_cur=grad_descent.x, x_prev=previous_x):\n",
    "    num_itter +=1\n",
    "    print('\\nItteration: {}'.format(num_itter))\n",
    "    \n",
    "    previous_x = grad_descent.x\n",
    "    grad_descent.zero_grad()\n",
    "    # Compute gradient\n",
    "    grad_descent.backward()\n",
    "    # x_k+1 = x_k + h_k\n",
    "    grad_descent.step()\n",
    "    grad_descent.info()\n",
    "    \n",
    "print('\\nConverged in {} itterations'.format(num_itter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
